{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f471752-33c5-4834-b7cd-8032ee87fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ac49d-a0fd-4da3-b395-55c5affd6364",
   "metadata": {},
   "source": [
    "## Training GPT-2 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d448fddb-6151-45a9-a22a-5df6fbe6706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseTrainer(transformers.Trainer):\n",
    "    def __init__(self, *args, sparsity_lambda=0.01, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sparsity_lambda = sparsity_lambda\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Compute the original loss\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        lm_logits = outputs.logits\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        loss = nn.CrossEntropyLoss()(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        # Add L1 regularization for sparsity\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss += self.sparsity_lambda * l1_norm\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09348cfb-7054-4e36-9a9b-e0bd7a1928ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lm(output_dir, num_train_epochs = 5, lr = 3e-5, per_device_train_batch_size = 6, save_steps = 10000, logging_dir = './logs', logging_steps = 500):\n",
    "    dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1', cache_dir = '/storage/vsub851/.cache')\n",
    "\n",
    "    #NOTE: If GPT-2 is too big for your GPU in colab, feel free to go smaller to [Distil-GPT2](https://huggingface.co/distilbert/distilgpt2) or something.\n",
    "    tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2', cache_dir = '/storage/vsub851/.cache')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    def prepare_data(examples):\n",
    "        tokenized_inputs = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "        tokenized_inputs['labels'] = tokenized_inputs['input_ids'].copy()\n",
    "        return tokenized_inputs\n",
    "    dataset = dataset.map(prepare_data, batched=True)\n",
    "\n",
    "    gpt2_config = transformers.GPT2Config.from_pretrained('gpt2', cache_dir = '/storage/vsub851/.cache')\n",
    "    model = transformers.AutoModelForCausalLM.from_config(gpt2_config)\n",
    "    trainer = SparseTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['validation'],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf18e443-8d7d-4104-9d62-468437a2a433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TODO: run language modeling training here\n",
    "output_dir = None ## IMPORTANT: this argument tells the trainer where to save the resultant model. Keep track of this in colab for example\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a899a2e7-59ac-4deb-b3e7-a284547fa5b3",
   "metadata": {},
   "source": [
    "## Your part: Get a linguistic feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a26765-2717-4f09-b168-99f2b1caa1ed",
   "metadata": {},
   "source": [
    "### Option 1: Get a pretrained model off the shelf \n",
    "Example: HuggingFace has several sentiment models that are trained for sentiment [here](https://huggingface.co/blog/sentiment-analysis-python). \n",
    "\n",
    "Feel free to find anything in this case. You can even ask how it work if you had a visual feature (do language models understand color. We can get pretty creative as long as we have a pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2034936-dc56-462c-af1d-a8ba8b208e8d",
   "metadata": {},
   "source": [
    "### Option 2: Train your own. \n",
    "Find a dataset and train your own. If you want to do that, I'll give you some example steps for dependencies.\n",
    "\n",
    "Check out the [syntax_parser](https://github.com/vsubramaniam851/multimeter-interp/tree/main/language_modeling/syntax_parser) to see the components of how to do this.\n",
    "\n",
    "First look at `ud_data.py` to see how we have to make a word level dataset for language.\n",
    "\n",
    "I make an example parser in `parser.py`.\n",
    "\n",
    "I train the parser for POS tagging in `pos_train.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecae95-a9fb-4750-81e4-4045fd42b2bc",
   "metadata": {},
   "source": [
    "## Multimeter Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ca88ebf-62be-417f-8c9a-53bca41737ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "674aba19-7a8e-466d-ab6d-69542e61923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hooks import *\n",
    "from multimeter import CombinedNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b42678ef-0924-46a7-bcb4-f9e53cc5fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModificationHook:\n",
    "    '''\n",
    "    Attaches hook to module associated with layer_index in pytorch. Applies a modification on this hook based on a passed in representation from \n",
    "    an external network in the hook function. Applies a weighted average of this with the current output of the module.\n",
    "    Inputs:\n",
    "        layer_index: Index to grab representations from. Set based on flattened leaf modules list designed by the network.\n",
    "        param: nn.Parameter set by pytorch and tuned to to incorporate the frozen output from another submodel with the current model.\n",
    "    '''\n",
    "    def __init__(self, layer_index, param):\n",
    "        self.layer_index = layer_index\n",
    "        self.frozen_output = None\n",
    "        self.handle = None\n",
    "        self.leaf_modules = []\n",
    "        self.param = param\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        #Weighted sum based on the sigmoid of the passed in parameter. \n",
    "        return (1-torch.sigmoid(self.param)) * output + torch.sigmoid(self.param) * self.frozen_output\n",
    "\n",
    "    def attach(self, model):\n",
    "        #Iterate over modules of pytorch. Some modules like nn.Sequential, nn.ModuleList have children so iterate and append those recursively.\n",
    "        def get_leaf_modules(module):\n",
    "            if not list(module.children()):  # if leaf node\n",
    "                self.leaf_modules.append(module)\n",
    "            for child in module.children():\n",
    "                get_leaf_modules(child)\n",
    "\n",
    "        get_leaf_modules(model)\n",
    "        #Register forward hook for the module that corresponds to the layer_index.\n",
    "        if self.layer_index < len(self.leaf_modules):\n",
    "            self.handle = self.leaf_modules[self.layer_index].register_forward_hook(self.hook_fn)\n",
    "        else:\n",
    "            raise IndexError(f'Layer index {self.layer_index} is out of range. Max index is {len(self.leaf_modules) - 1}')\n",
    "\n",
    "    def remove(self):\n",
    "        if self.handle is not None:\n",
    "            self.handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9037807a-eea3-4696-99ca-ae10601e8544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedNetwork(nn.Module):\n",
    "    '''\n",
    "    Multimeter network. Takes an original network and trains with input from a frozen network. \n",
    "    Inputs:\n",
    "        original_network: Optimized network\n",
    "        frozen_network: Multimeter network that original network can steal weights from\n",
    "        representation_layer_index: start layer index to connect multimeter to.\n",
    "        modification_layer_index: end layer index to combine frozen output and original network output.\n",
    "    '''\n",
    "    def __init__(self, original_network, frozen_network, representation_layer_index, modification_layer_index):\n",
    "        super().__init__()\n",
    "        self.original_network = original_network\n",
    "        self.frozen_network = frozen_network\n",
    "        self.parameter = nn.Parameter(torch.randn(()))\n",
    "        self.representation_hook = RepresentationHook(representation_layer_index)\n",
    "        self.modification_hook = ModificationHook(modification_layer_index, self.parameter)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Attach representation hook to get output of start module\n",
    "        self.representation_hook.attach(self.original_network)\n",
    "        #Run original network all the way through to get the output.\n",
    "        with torch.no_grad():\n",
    "            original_output = self.original_network(x)\n",
    "        #Get frozen output from multimeter network\n",
    "        frozen_output = self.frozen_network(self.representation_hook.representation)\n",
    "        #Set modification hook frozen output\n",
    "        self.modification_hook.frozen_output = frozen_output\n",
    "        #Attach modification hook to the network\n",
    "        self.modification_hook.attach(self.original_network)\n",
    "        #Run through original network again with hook to recombine the entire frozen output\n",
    "        final_output = self.original_network(x)\n",
    "        #Remove hooks from the network.\n",
    "        self.representation_hook.remove()\n",
    "        self.modification_hook.remove()\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6d0219e-73d4-4acc-b1a6-4222eeec3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFCombinedNetwork(CombinedNetwork):\n",
    "    '''\n",
    "    HuggingFace based multimeter network. Overloads the original multimeter network.\n",
    "    '''\n",
    "    def forward(self, input_ids, attention_mask = None, labels = None, **kwargs):\n",
    "        #Attach representation hook to get output of start module\n",
    "        self.representation_hook.attach(self.original_network)\n",
    "        #Run original network all the way through to get the output.\n",
    "        with torch.no_grad():\n",
    "            original_output = self.original_network(input_ids)\n",
    "        #Get frozen output from multimeter network\n",
    "        frozen_output = self.frozen_network(self.representation_hook.representation)\n",
    "        #Set modification hook frozen output\n",
    "        self.modification_hook.frozen_output = frozen_output\n",
    "        #Attach modification hook to the network\n",
    "        self.modification_hook.attach(self.original_network)\n",
    "        #Run through original network again with hook to recombine the entire frozen output\n",
    "        final_output = self.original_network(input_ids)\n",
    "        #Remove hooks from the network.\n",
    "        self.representation_hook.remove()\n",
    "        self.modification_hook.remove()\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23b83817-e128-4b73-a8cb-45e3e4d9cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need a sparse trainer for fancy saving. I'm too lazy to make the code clean for this...\n",
    "class SparseTrainer(transformers.Trainer):\n",
    "    def __init__(self, *args, sparsity_lambda=0.01, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sparsity_lambda = sparsity_lambda\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Compute the original loss\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        lm_logits = outputs.logits\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        loss = nn.CrossEntropyLoss()(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        # Add L1 regularization for sparsity\n",
    "        l1_norm = sum(p.abs().sum() for p in model.original_network.parameters())\n",
    "        loss += self.sparsity_lambda * l1_norm\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def _save(self, output_dir = None, state_dict = None):\n",
    "        if output_dir is None:\n",
    "            output_dir = self.args.output_dir\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok = True)\n",
    "        model = self.model_wrapped if hasattr(self, 'model_wrapped') else self.model\n",
    "        model.original_network.save_pretrained(os.path.join(output_dir, 'original_network'))\n",
    "\n",
    "        torch.save(model.frozen_network, os.path.join(output_dir, 'frozen_network'))\n",
    "        torch.save(model.parameter, os.path.join(output_dir, 'parameter.pt'))\n",
    "\n",
    "        torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "        if self.optimizer and self.lr_scheduler:\n",
    "            torch.save(\n",
    "                {\n",
    "                    'optimizer': self.optimizer.state_dict(),\n",
    "                    'lr_scheduler': self.lr_scheduler.state_dict(),\n",
    "                },\n",
    "                os.path.join(output_dir, 'optimizer.pt'),\n",
    "            )\n",
    "\n",
    "        self.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540f7d9-df09-49f3-8d56-a6bed9255303",
   "metadata": {},
   "source": [
    "Below, I show you what an example Frozen/Multimeter network looks like. \n",
    "TODO: You need to replace this with your own. Most of the structure is the same. I will show you where to add your part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbb3f793-4476-4615-bfb8-6805d93e55ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntaxFrozenNetwork(nn.Module):\n",
    "    def __init__(self, in_features, model_in_features, model_out_features, out_features, model):\n",
    "        #CHANGE MODEL TO YOUR MODEL AND SET YOUR PARAMETERS\n",
    "        super(SyntaxFrozenNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features, model_in_features)\n",
    "        self.linear2 = nn.Linear(model_out_features, out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.model = model\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.linear1(x)\n",
    "        x, _, _ = self.model(inputs = None, attention_mask = None, inputs_embeds = x)\n",
    "        #NOTE: A quick note to self; we choose to send logits instead of representations for the reason that \n",
    "        # representations may have added information/capacity when using BERT/GPT-2. It's super important to NOT\n",
    "        # let this added capacity ablate more weights of the network. So we only use the logits instead.\n",
    "\n",
    "        # Another quick note to self: Using BERT/GPT-2 isn't a foolproof idea so I need to think about this more.\n",
    "        # I guess this was harder than I thought...\n",
    "        x = x.view(x.shape[0], x.shape[1] * x.shape[2])\n",
    "        x = self.relu(self.linear2(x))\n",
    "        # Note this may change! Don't assume 768. Be sure to debug this\n",
    "        x = x.view(x.shape[0], -1, 768)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87199cca-064c-48f4-96c8-4ef8656ab756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_multimeter(output_dir, pretrained_dir, mul_network, frozen_dir, index1, index2, use_pretrained = True, num_train_epochs = 5, lr = 3e-5, per_device_train_batch_size = 4, save_steps = 10000, logging_dir = './logs', logging_steps = 500):\n",
    "    # Set up original network\n",
    "    if use_pretrained:\n",
    "        gpt2_model = transformers.AutoModelForCausalLM.from_pretrained(pretrained_dir)\n",
    "    else:\n",
    "        # Sanity Check: train from a randomly initialized network instead\n",
    "        gpt2_config = transformers.AutoConfig.from_pretrained('gpt2', cache_dir = '/storage/vsub851/.cache')\n",
    "        gpt2_model = transformers.AutoModelForCausalLM.from_config(gpt2_config)\n",
    "\n",
    "    # Set up frozen network. For Aliya: THIS WILL CHANGE.\n",
    "    bert_model = transformers.AutoModel.from_pretrained('bert-base-uncased', cache_dir = '/storage/vsub851/.cache')\n",
    "    # NOTE: 128 refers to the max length from the model. TODO: Add these as parameters\n",
    "    if mul_network == 'dep':\n",
    "        dep_model = PairwiseMLP(bert_model, in_features = 768, hidden_state = 5)\n",
    "        dep_model.load_state_dict(torch.load(frozen_dir))\n",
    "        frozen_network = SyntaxFrozenNetwork(in_features = 768, model_in_features = 768, model_out_features = 128 * 128, out_features = 128 * 768, model = dep_model)\n",
    "    elif mul_network == 'pos':\n",
    "        ckpt = torch.load(frozen_dir)\n",
    "        num_pos = ckpt['linear2.weight'].shape[0]\n",
    "        pos_model = MLP(bert_model, in_features = 768, hidden_state = 5, out_features = num_pos)\n",
    "        pos_model.load_state_dict(torch.load(frozen_dir))\n",
    "        frozen_network = SyntaxFrozenNetwork(in_features = 768, model_in_features = 768, model_out_features = 128 * num_pos, out_features = 128 * 768, model = pos_model)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Set up combined network\n",
    "    combined_network = HFCombinedNetwork(gpt2_model, frozen_network, representation_layer_index = index1, modification_layer_index = index2)\n",
    "\n",
    "    # Set up dataset from wikitext again\n",
    "    dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1', cache_dir = '/storage/vsub851/.cache')\n",
    "    tokenizer = transformers.GPT2Tokenizer.from_pretrained(pretrained_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Function to add labels for language modeling\n",
    "    def prepare_data(examples):\n",
    "        tokenized_inputs = tokenizer(examples['text'], truncation = True, padding = 'max_length', max_length = 128)\n",
    "        tokenized_inputs['labels'] = tokenized_inputs['input_ids'].copy()\n",
    "        return tokenized_inputs\n",
    "\n",
    "    dataset = dataset.map(prepare_data, batched=True)\n",
    "\n",
    "    training_args = transformers.TrainingArguments(\n",
    "        output_dir = output_dir,\n",
    "        overwrite_output_dir = True,\n",
    "        num_train_epochs = num_train_epochs,\n",
    "        per_device_train_batch_size = per_device_train_batch_size,\n",
    "        learning_rate = lr,\n",
    "        save_steps = save_steps,\n",
    "        logging_dir = logging_dir,\n",
    "        logging_steps = logging_steps\n",
    "    )\n",
    "\n",
    "    trainer = SparseTrainer(\n",
    "        model=combined_network,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['validation'],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    print('Beginning training...')\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c01acc-6e8b-4629-97f0-8d70eb542212",
   "metadata": {},
   "source": [
    "Run everything here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68b7f692-8301-48f1-834b-a08e54108594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm_multimeter(...)\n",
    "## IMPORTANT ARGUMENT: representation_layer_index = where the multimeter starts, modification_layer_index = where the multimeter ends. These must be different values and representation_layer_index < modification_layer_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe836509-29e2-4c03-bd1b-e9dc64aaf312",
   "metadata": {},
   "source": [
    "## Analysis Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb09ee3-3bac-455f-80d6-2b95a0b29181",
   "metadata": {},
   "source": [
    "If you get everything to run, we can talk about this :p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f30c4-8937-4ef5-b9f8-63b906383a37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
